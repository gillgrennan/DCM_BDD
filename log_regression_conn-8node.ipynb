{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5d62ae72",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "1st method: iteratively trying top features and testing model prediction accuracy \n",
    "2nd method: recursive feature selection \n",
    "3rd method: sequential feature selection (both forward / backward) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8cb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a8dc962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy using all connectivities + MH data: 0.8571428571428571\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your data in a Pandas DataFrame called 'data'\n",
    "# where the first 4 columns are your features and the last column is the target variable.\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-2]  # all columns except 1st/last are features\n",
    "y = data.iloc[:, -2]   # Last column is the target variable\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the logistic regression model\n",
    "logreg = LogisticRegression(random_state = 42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg, X_train, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"Prediction Accuracy using all connectivities + MH data:\", accuracy_logreg)\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4c1fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Top Features and Importance Scores:\n",
      "depression: 0.7728860160234696\n",
      "anxiety: 0.6353272020887974\n",
      "LeftAmy-LeftPul: 0.5182114308617533\n",
      "RightPul-RightFFA: 0.43559666041015616\n"
     ]
    }
   ],
   "source": [
    "## feature selection -- reducing number of features to top importance \n",
    "\n",
    "# Step 4: Get the coefficients (feature importances) of the model\n",
    "feature_importances = np.abs(logreg.coef_[0])  # Take the absolute values to handle negative coefficients\n",
    "\n",
    "# Step 5: Select the top features with the highest importance/amount of covariance explained \n",
    "num_top_features = 4 # Change this number to select a different number of top features\n",
    "top_feature_indices = np.argsort(feature_importances)[::-1][:num_top_features]\n",
    "top_features = X.columns[top_feature_indices]\n",
    "\n",
    "# Print the selected top features and covariance explained by feature \n",
    "print(\"Selected Top Features and Importance Scores:\")\n",
    "for feature, importance in zip(top_features, feature_importances[top_feature_indices]):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d713eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.86\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Retrain the logistic regression model using only the selected top features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "logreg_top_features = LogisticRegression()\n",
    "logreg_top_features.fit(X_train_top, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_top_features, X_train_top, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_top = logreg_top_features.predict(X_test_top)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred_top)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_top_features.score(X_test_top, y_test)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a817b9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance explained by each feature:\n",
      "depression: 0.771328048639235\n",
      "anxiety: 0.6256402425562101\n",
      "Accuracy of logistic regression classifier on test set: 0.86\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "     ..\n",
       "64    0\n",
       "65    0\n",
       "66    0\n",
       "67    0\n",
       "68    0\n",
       "Name: group, Length: 69, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing model accuracy of just Mental Health data (anxiety / depression ratings )\n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_mhonly')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-3]  # all columns except 1st/last are features\n",
    "y = data.iloc[:, -3]   # Last column is the target variable\n",
    "\n",
    "X_train_mh, X_test_mh, y_train_mh, y_test_mh = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg_mh = LogisticRegression()\n",
    "logreg_mh.fit(X_train_mh, y_train_mh)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_mh, X_train_mh, y_train_mh, cv=num_folds)\n",
    "average_accuracy_mh = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_mh = logreg_mh.predict(X_test_mh)\n",
    "accuracy_logreg_mh = accuracy_score(y_test_mh,y_pred_mh)\n",
    "\n",
    "## feature selection -- reducing number of features to top importance \n",
    "\n",
    "# Step 4: Get the coefficients (feature importances) of the model\n",
    "feature_importances_mh = np.abs(logreg_mh.coef_[0])  # Take the absolute values to handle negative coefficients\n",
    "\n",
    "# Step 5: Select the top features with the highest importance/amount of covariance explained \n",
    "num_top_features = 2 # Change this number to select a different number of top features\n",
    "top_feature_indices_mh = np.argsort(feature_importances_mh)[::-1][:num_top_features]\n",
    "top_features_mh = X.columns[top_feature_indices_mh]\n",
    "\n",
    "# Print the selected top features and covariance explained by feature \n",
    "print(\"Covariance explained by each feature:\")\n",
    "for feature, importance in zip(top_features_mh, feature_importances_mh[top_feature_indices_mh]):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_mh.score(X_test_mh, y_test_mh)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy_mh:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "923f410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.86\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "     ..\n",
       "64    0\n",
       "65    0\n",
       "66    0\n",
       "67    0\n",
       "68    0\n",
       "Name: group, Length: 69, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing model accuracy of mh data (anx/depression) + significantly different \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_limitedconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-2]  # all columns except 1st/last are features\n",
    "y = data.iloc[:, -2]   # Last column is the target variable\n",
    "\n",
    "X_train_mhconn2, X_test_mhconn2, y_train_mhconn2, y_test_mhconn2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg_mhconn2 = LogisticRegression()\n",
    "logreg_mhconn2.fit(X_train_mhconn2, y_train_mhconn2)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_mhconn2, X_train_mhconn2, y_train_mhconn2, cv=num_folds)\n",
    "average_accuracy_mhconn2 = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_mhconn2 = logreg_mhconn2.predict(X_test_mhconn2)\n",
    "accuracy_logreg_mhconn2 = accuracy_score(y_test_mhconn2,y_pred_mhconn2)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_mhconn2.score(X_test_mhconn2, y_test_mhconn2)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy_mhconn2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01bae5",
   "metadata": {},
   "source": [
    "# Method 2 -- recursive feature elimination w k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6e7b307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 2\n",
      "Features selected by forward sequential selection: ['LeftAmy-LeftPul' 'depression']\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination with cross-validation \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "\n",
    "feature_names = X.columns.values\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-1]  # All but first / last column are features\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[rfecv.get_support()]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "614e36d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 1\n",
      "Features selected by forward sequential selection: ['depression']\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination with cross-validation -- mh only  \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_mhonly')\n",
    "\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-1]  # All but first / last column are features\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "feature_names = X.columns.values\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[rfecv.get_support()]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3f276546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 1\n",
      "Features selected by forward sequential selection: ['depression']\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination with cross-validation -- mh + significantly diff connectivies   \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_limitedconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-1]  # All but first / last column are features\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "feature_names = X.columns.values\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[rfecv.get_support()]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050aa94",
   "metadata": {},
   "source": [
    "# Method 3: sequential feature selection (both forward / backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "08e44e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by forward sequential selection: ['LeftAmy-LeftAmy' 'LeftAmy-LeftPul' 'depression']\n",
      "Done in 1.527s\n",
      "Features selected by backward sequential selection: ['RightPul-RightAmy' 'RightPul-RightPul' 'depression']\n",
      "Done in 10.536s\n"
     ]
    }
   ],
   "source": [
    "## # sequential feature selector \n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "X = data.iloc[:, 1:-1]  # 2nd - 2nd to end columns are features (ignoring subjectname + group)\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "feature_names = X.columns.values\n",
    "\n",
    "logreg1 = LogisticRegression()\n",
    "#logreg_ridge = RidgeCV(logreg1.fit(X,y))\n",
    "# starts with no features and adds one by one \n",
    "tic_fwd = time()\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    logreg1, n_features_to_select=3, direction=\"forward\"\n",
    ").fit(X, y)\n",
    "toc_fwd = time()\n",
    "\n",
    "# backwards -- starts w all features and slowly adds each one \n",
    "tic_bwd = time()\n",
    "sfs_backward = SequentialFeatureSelector(\n",
    "    logreg1, n_features_to_select=3, direction=\"backward\"\n",
    ").fit(X, y)\n",
    "toc_bwd = time()\n",
    "\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[sfs_forward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "print(\n",
    "    \"Features selected by backward sequential selection: \"\n",
    "    f\"{feature_names[sfs_backward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80531e73",
   "metadata": {},
   "source": [
    "# Logistic regression for mental health data (getting stats)\n",
    "Averaged anxiety / depression scores since they were so correlated -- median = 4, marked all participants that were 4 or above as 1 (high dep / anxiety group) and all that were below 4 as 0 (low dep/anx group) \n",
    "\n",
    "want to see conn ability to predict high/low dep/anx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54b0d130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.29\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.61\n"
     ]
    }
   ],
   "source": [
    "# testing model accuracy of mh data (anx/depression) + significantly different connectivites \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_limitedconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-4]  # just connectivities \n",
    "y = data.iloc[:, -1]   # Last column is the target variable -- mh group \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg, X_train, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78bc3f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88785719 0.25673109 0.67272688]\n",
      "const                0.416976\n",
      "LeftAmy-LeftmFOC     0.025669\n",
      "RightAmy-RightPul    0.170549\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilliangrennan/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>mh_group</td>     <th>  No. Observations:  </th>  <td>    69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    66</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 31 Jul 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.07758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:30:06</td>     <th>  Log-Likelihood:    </th> <td> -43.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -47.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.02482</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -0.2563</td> <td>    0.316</td> <td>   -0.812</td> <td> 0.417</td> <td>   -0.875</td> <td>    0.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftAmy-LeftmFOC</th>  <td>    2.5942</td> <td>    1.163</td> <td>    2.231</td> <td> 0.026</td> <td>    0.315</td> <td>    4.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightAmy-RightPul</th> <td>   -2.2215</td> <td>    1.621</td> <td>   -1.370</td> <td> 0.171</td> <td>   -5.399</td> <td>    0.956</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               mh_group   No. Observations:                   69\n",
       "Model:                          Logit   Df Residuals:                       66\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Mon, 31 Jul 2023   Pseudo R-squ.:                 0.07758\n",
       "Time:                        12:30:06   Log-Likelihood:                -43.950\n",
       "converged:                       True   LL-Null:                       -47.646\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.02482\n",
       "=====================================================================================\n",
       "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                -0.2563      0.316     -0.812      0.417      -0.875       0.363\n",
       "LeftAmy-LeftmFOC      2.5942      1.163      2.231      0.026       0.315       4.873\n",
       "RightAmy-RightPul    -2.2215      1.621     -1.370      0.171      -5.399       0.956\n",
       "=====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_limitedconns')\n",
    "X = data.iloc[:, 1:-4]  # just connectivities \n",
    "y = data.iloc[:, -1]   # Last column is the target variable -- mh group \n",
    "\n",
    "def logit_pvalue(model, x):\n",
    "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "    parameters:\n",
    "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "        x:     matrix on which the model was fit\n",
    "    This function uses asymtptics for maximum likelihood estimates.\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(logit_pvalue(model, X))\n",
    "\n",
    "# compare with statsmodels\n",
    "import statsmodels.api as sm\n",
    "sm_model = sm.Logit(y, sm.add_constant(X)).fit(disp=0)\n",
    "print(sm_model.pvalues)\n",
    "sm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30c66bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.21\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.66\n"
     ]
    }
   ],
   "source": [
    "# testing model accuracy of mh data (anx/depression) + significantly different connectivites \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-4]  # just connectivities \n",
    "y = data.iloc[:, -1]   # Last column is the target variable -- mh group \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg, X_train, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7e51eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97492716 0.96638671 0.60531765 0.50912767 0.70797272 0.95523025\n",
      " 0.83401453 0.98315302 0.81286307 0.78408757 0.92619781 0.97789254\n",
      " 0.87503909 0.97971589 0.92440409 0.92665707 0.98189847 0.91703737\n",
      " 0.92750981 0.90437824 0.85565802 0.95747752 0.83096205 0.77966446\n",
      " 0.99237437]\n",
      "const                  0.627265\n",
      "LeftAmy-LeftAmy        0.941911\n",
      "LeftAmy-LeftFFA        0.147792\n",
      "LeftAmy-LeftmFOC       0.037179\n",
      "LeftAmy-LeftPul        0.009778\n",
      "RightAmy-RightAmy      0.542987\n",
      "RightAmy-RightFFA      0.976901\n",
      "RightAmy-RightmFOC     0.846783\n",
      "RightAmy-RightPul      0.420621\n",
      "LeftFFA-LeftAmy        0.983108\n",
      "LeftFFA-LeftFFA        0.688643\n",
      "LeftFFA-LeftPul        0.702738\n",
      "RightFFA-RightAmy      0.558136\n",
      "RightFFA-RightFFA      0.956026\n",
      "RightFFA-RightPul      0.844089\n",
      "LeftmOFC-LeftAmy       0.483366\n",
      "LeftmOFC-LeftmOFC      0.715417\n",
      "RightmOFC-RightAmy     0.957264\n",
      "RightmOFC-RightmOFC    0.059791\n",
      "LeftPul-LeftAmy        0.968507\n",
      "LeftPul-LeftFFA        0.365575\n",
      "LeftPul-LeftPul        0.195653\n",
      "RightPul-RightAmy      0.218131\n",
      "RightPul-RightFFA      0.515455\n",
      "RightPul-RightPul      0.707990\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilliangrennan/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>mh_group</td>     <th>  No. Observations:  </th>  <td>    69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    44</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    24</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 31 Jul 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.3223</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:32:48</td>     <th>  Log-Likelihood:    </th> <td> -32.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -47.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.1622</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>               <td>    0.5177</td> <td>    1.066</td> <td>    0.486</td> <td> 0.627</td> <td>   -1.572</td> <td>    2.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftAmy-LeftAmy</th>     <td>   -0.5851</td> <td>    8.030</td> <td>   -0.073</td> <td> 0.942</td> <td>  -16.323</td> <td>   15.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftAmy-LeftFFA</th>     <td>   -3.5033</td> <td>    2.420</td> <td>   -1.447</td> <td> 0.148</td> <td>   -8.247</td> <td>    1.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftAmy-LeftmFOC</th>    <td>    5.9262</td> <td>    2.844</td> <td>    2.084</td> <td> 0.037</td> <td>    0.352</td> <td>   11.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftAmy-LeftPul</th>     <td>   -7.2637</td> <td>    2.811</td> <td>   -2.584</td> <td> 0.010</td> <td>  -12.774</td> <td>   -1.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightAmy-RightAmy</th>   <td>    3.5532</td> <td>    5.841</td> <td>    0.608</td> <td> 0.543</td> <td>   -7.895</td> <td>   15.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightAmy-RightFFA</th>   <td>   -0.0593</td> <td>    2.047</td> <td>   -0.029</td> <td> 0.977</td> <td>   -4.072</td> <td>    3.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightAmy-RightmFOC</th>  <td>   -0.4080</td> <td>    2.111</td> <td>   -0.193</td> <td> 0.847</td> <td>   -4.546</td> <td>    3.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightAmy-RightPul</th>   <td>   -2.6151</td> <td>    3.247</td> <td>   -0.805</td> <td> 0.421</td> <td>   -8.980</td> <td>    3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftFFA-LeftAmy</th>     <td>    0.0426</td> <td>    2.014</td> <td>    0.021</td> <td> 0.983</td> <td>   -3.904</td> <td>    3.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftFFA-LeftFFA</th>     <td>   -1.7642</td> <td>    4.403</td> <td>   -0.401</td> <td> 0.689</td> <td>  -10.393</td> <td>    6.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftFFA-LeftPul</th>     <td>   -0.9789</td> <td>    2.565</td> <td>   -0.382</td> <td> 0.703</td> <td>   -6.006</td> <td>    4.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightFFA-RightAmy</th>   <td>   -1.6141</td> <td>    2.756</td> <td>   -0.586</td> <td> 0.558</td> <td>   -7.016</td> <td>    3.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightFFA-RightFFA</th>   <td>   -0.2472</td> <td>    4.483</td> <td>   -0.055</td> <td> 0.956</td> <td>   -9.033</td> <td>    8.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightFFA-RightPul</th>   <td>    0.4667</td> <td>    2.373</td> <td>    0.197</td> <td> 0.844</td> <td>   -4.184</td> <td>    5.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftmOFC-LeftAmy</th>    <td>    3.8439</td> <td>    5.484</td> <td>    0.701</td> <td> 0.483</td> <td>   -6.905</td> <td>   14.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftmOFC-LeftmOFC</th>   <td>   -3.0707</td> <td>    8.422</td> <td>   -0.365</td> <td> 0.715</td> <td>  -19.578</td> <td>   13.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightmOFC-RightAmy</th>  <td>   -0.2505</td> <td>    4.674</td> <td>   -0.054</td> <td> 0.957</td> <td>   -9.411</td> <td>    8.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightmOFC-RightmOFC</th> <td>   13.6106</td> <td>    7.231</td> <td>    1.882</td> <td> 0.060</td> <td>   -0.561</td> <td>   27.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftPul-LeftAmy</th>     <td>    0.1036</td> <td>    2.624</td> <td>    0.039</td> <td> 0.969</td> <td>   -5.039</td> <td>    5.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftPul-LeftFFA</th>     <td>   -1.4670</td> <td>    1.621</td> <td>   -0.905</td> <td> 0.366</td> <td>   -4.645</td> <td>    1.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftPul-LeftPul</th>     <td>    9.1595</td> <td>    7.078</td> <td>    1.294</td> <td> 0.196</td> <td>   -4.714</td> <td>   23.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightPul-RightAmy</th>   <td>   -3.1554</td> <td>    2.562</td> <td>   -1.232</td> <td> 0.218</td> <td>   -8.177</td> <td>    1.866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightPul-RightFFA</th>   <td>    1.4246</td> <td>    2.190</td> <td>    0.650</td> <td> 0.515</td> <td>   -2.869</td> <td>    5.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightPul-RightPul</th>   <td>   -2.5334</td> <td>    6.764</td> <td>   -0.375</td> <td> 0.708</td> <td>  -15.790</td> <td>   10.723</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               mh_group   No. Observations:                   69\n",
       "Model:                          Logit   Df Residuals:                       44\n",
       "Method:                           MLE   Df Model:                           24\n",
       "Date:                Mon, 31 Jul 2023   Pseudo R-squ.:                  0.3223\n",
       "Time:                        12:32:48   Log-Likelihood:                -32.289\n",
       "converged:                       True   LL-Null:                       -47.646\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1622\n",
       "=======================================================================================\n",
       "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------\n",
       "const                   0.5177      1.066      0.486      0.627      -1.572       2.607\n",
       "LeftAmy-LeftAmy        -0.5851      8.030     -0.073      0.942     -16.323      15.153\n",
       "LeftAmy-LeftFFA        -3.5033      2.420     -1.447      0.148      -8.247       1.241\n",
       "LeftAmy-LeftmFOC        5.9262      2.844      2.084      0.037       0.352      11.500\n",
       "LeftAmy-LeftPul        -7.2637      2.811     -2.584      0.010     -12.774      -1.753\n",
       "RightAmy-RightAmy       3.5532      5.841      0.608      0.543      -7.895      15.002\n",
       "RightAmy-RightFFA      -0.0593      2.047     -0.029      0.977      -4.072       3.953\n",
       "RightAmy-RightmFOC     -0.4080      2.111     -0.193      0.847      -4.546       3.730\n",
       "RightAmy-RightPul      -2.6151      3.247     -0.805      0.421      -8.980       3.749\n",
       "LeftFFA-LeftAmy         0.0426      2.014      0.021      0.983      -3.904       3.990\n",
       "LeftFFA-LeftFFA        -1.7642      4.403     -0.401      0.689     -10.393       6.865\n",
       "LeftFFA-LeftPul        -0.9789      2.565     -0.382      0.703      -6.006       4.048\n",
       "RightFFA-RightAmy      -1.6141      2.756     -0.586      0.558      -7.016       3.788\n",
       "RightFFA-RightFFA      -0.2472      4.483     -0.055      0.956      -9.033       8.539\n",
       "RightFFA-RightPul       0.4667      2.373      0.197      0.844      -4.184       5.118\n",
       "LeftmOFC-LeftAmy        3.8439      5.484      0.701      0.483      -6.905      14.593\n",
       "LeftmOFC-LeftmOFC      -3.0707      8.422     -0.365      0.715     -19.578      13.437\n",
       "RightmOFC-RightAmy     -0.2505      4.674     -0.054      0.957      -9.411       8.910\n",
       "RightmOFC-RightmOFC    13.6106      7.231      1.882      0.060      -0.561      27.783\n",
       "LeftPul-LeftAmy         0.1036      2.624      0.039      0.969      -5.039       5.246\n",
       "LeftPul-LeftFFA        -1.4670      1.621     -0.905      0.366      -4.645       1.711\n",
       "LeftPul-LeftPul         9.1595      7.078      1.294      0.196      -4.714      23.033\n",
       "RightPul-RightAmy      -3.1554      2.562     -1.232      0.218      -8.177       1.866\n",
       "RightPul-RightFFA       1.4246      2.190      0.650      0.515      -2.869       5.718\n",
       "RightPul-RightPul      -2.5334      6.764     -0.375      0.708     -15.790      10.723\n",
       "=======================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(logit_pvalue(model, X))\n",
    "\n",
    "# compare with statsmodels\n",
    "import statsmodels.api as sm\n",
    "sm_model = sm.Logit(y, sm.add_constant(X)).fit(disp=0)\n",
    "print(sm_model.pvalues)\n",
    "sm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7d76e",
   "metadata": {},
   "source": [
    "# Just using connectivities (no mental health data included as features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d55a5991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy using all connectivities + MH data: 0.42857142857142855\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your data in a Pandas DataFrame called 'data'\n",
    "# where the first 4 columns are your features and the last column is the target variable.\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-4]  # all columns except 1st/last are features\n",
    "y = data.iloc[:, -2]   # Last column is the target variable\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the logistic regression model\n",
    "logreg = LogisticRegression(random_state = 42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg, X_train, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"Prediction Accuracy using all connectivities + MH data:\", accuracy_logreg)\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84173779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.43\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.67\n"
     ]
    }
   ],
   "source": [
    "# testing model accuracy of mh data (anx/depression) + significantly different \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_limitedconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-4]  # all columns except 1st/last are features\n",
    "y = data.iloc[:, -2]   # Last column is the target variable\n",
    "\n",
    "X_train_mhconn2, X_test_mhconn2, y_train_mhconn2, y_test_mhconn2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg_mhconn2 = LogisticRegression()\n",
    "logreg_mhconn2.fit(X_train_mhconn2, y_train_mhconn2)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_mhconn2, X_train_mhconn2, y_train_mhconn2, cv=num_folds)\n",
    "average_accuracy_mhconn2 = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_mhconn2 = logreg_mhconn2.predict(X_test_mhconn2)\n",
    "accuracy_logreg_mhconn2 = accuracy_score(y_test_mhconn2,y_pred_mhconn2)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_mhconn2.score(X_test_mhconn2, y_test_mhconn2)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy_mhconn2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "823bb0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 8\n",
      "Features selected by forward sequential selection: ['LeftAmy-LeftFFA' 'LeftAmy-LeftmFOC' 'RightAmy-RightPul'\n",
      " 'LeftFFA-LeftAmy' 'LeftFFA-LeftFFA' 'RightmOFC-RightAmy'\n",
      " 'LeftPul-LeftPul' 'RightPul-RightAmy']\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination with cross-validation \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-4]  # All but first / last column are features\n",
    "y = data.iloc[:, -2]   # Last column is the target variable\n",
    "feature_names = X.columns.values\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[rfecv.get_support()]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6b6c2791",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy using 10-fold Cross-Validation using all 26 connectivities: 0.58\n",
      "\n",
      "Selected Top Features and Importance Scores:\n",
      "LeftAmy-LeftmFOC: 0.9634015004586973\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation just using top 2 features: 0.60\n",
      "[0.26576606 0.27816265]\n",
      "const               0.083143\n",
      "LeftAmy-LeftmFOC    0.037023\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilliangrennan/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>group</td>      <th>  No. Observations:  </th>  <td>    69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    67</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 31 Jul 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.05219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:38:32</td>     <th>  Log-Likelihood:    </th> <td> -45.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -47.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.02574</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>   -0.5621</td> <td>    0.324</td> <td>   -1.733</td> <td> 0.083</td> <td>   -1.198</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftAmy-LeftmFOC</th> <td>    2.3504</td> <td>    1.127</td> <td>    2.086</td> <td> 0.037</td> <td>    0.141</td> <td>    4.559</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  group   No. Observations:                   69\n",
       "Model:                          Logit   Df Residuals:                       67\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Mon, 31 Jul 2023   Pseudo R-squ.:                 0.05219\n",
       "Time:                        13:38:32   Log-Likelihood:                -45.159\n",
       "converged:                       True   LL-Null:                       -47.646\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.02574\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const               -0.5621      0.324     -1.733      0.083      -1.198       0.074\n",
       "LeftAmy-LeftmFOC     2.3504      1.127      2.086      0.037       0.141       4.559\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding top 2 features \n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-4]  # all columns except 1st/last are features\n",
    "y = data.iloc[:, -2]   # Last column is the target variable\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the logistic regression model\n",
    "logreg = LogisticRegression(random_state = 42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg, X_train, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation using all 26 connectivities: {average_accuracy:.2f}\")\n",
    "## feature selection -- reducing number of features to top importance \n",
    "\n",
    "# Step 4: Get the coefficients (feature importances) of the model\n",
    "feature_importances = np.abs(logreg.coef_[0])  # Take the absolute values to handle negative coefficients\n",
    "\n",
    "# Step 5: Select the top features with the highest importance/amount of covariance explained \n",
    "num_top_features = 1 # Change this number to select a different number of top features\n",
    "top_feature_indices = np.argsort(feature_importances)[::-1][:num_top_features]\n",
    "top_features = X.columns[top_feature_indices]\n",
    "\n",
    "# Print the selected top features and covariance explained by feature \n",
    "print(f\"\\nSelected Top Features and Importance Scores:\")\n",
    "for feature, importance in zip(top_features, feature_importances[top_feature_indices]):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "    \n",
    "# Step 6: Retrain the logistic regression model using only the selected top features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "logreg_top_features = LogisticRegression()\n",
    "logreg_top_features.fit(X_train_top, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_top_features, X_train_top, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_top = logreg_top_features.predict(X_test_top)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred_top)\n",
    "\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation just using top 2 features: {average_accuracy:.2f}\")\n",
    "\n",
    "X_top = X[top_features]\n",
    "logreg_top_features = LogisticRegression()\n",
    "logreg_top_features.fit(X_top, y)\n",
    "\n",
    "def logit_pvalue(model, x):\n",
    "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "    parameters:\n",
    "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "        x:     matrix on which the model was fit\n",
    "    This function uses asymtptics for maximum likelihood estimates.\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "print(logit_pvalue(logreg_top_features, X_top))\n",
    "\n",
    "# compare with statsmodels\n",
    "import statsmodels.api as sm\n",
    "sm_model = sm.Logit(y, sm.add_constant(X_top)).fit(disp=0)\n",
    "print(sm_model.pvalues)\n",
    "sm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b795a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27813239 0.27353473 0.50524946]\n",
      "const                0.079426\n",
      "LeftAmy-LeftmFOC     0.026012\n",
      "RightAmy-RightPul    0.038401\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilliangrennan/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>group</td>      <th>  No. Observations:  </th>  <td>    69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    66</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 31 Jul 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.1083</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:01:52</td>     <th>  Log-Likelihood:    </th> <td> -42.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -47.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.005728</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -0.5874</td> <td>    0.335</td> <td>   -1.754</td> <td> 0.079</td> <td>   -1.244</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LeftAmy-LeftmFOC</th>  <td>    2.6658</td> <td>    1.198</td> <td>    2.226</td> <td> 0.026</td> <td>    0.319</td> <td>    5.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RightAmy-RightPul</th> <td>   -3.9720</td> <td>    1.918</td> <td>   -2.071</td> <td> 0.038</td> <td>   -7.732</td> <td>   -0.212</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  group   No. Observations:                   69\n",
       "Model:                          Logit   Df Residuals:                       66\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Mon, 31 Jul 2023   Pseudo R-squ.:                  0.1083\n",
       "Time:                        13:01:52   Log-Likelihood:                -42.483\n",
       "converged:                       True   LL-Null:                       -47.646\n",
       "Covariance Type:            nonrobust   LLR p-value:                  0.005728\n",
       "=====================================================================================\n",
       "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                -0.5874      0.335     -1.754      0.079      -1.244       0.069\n",
       "LeftAmy-LeftmFOC      2.6658      1.198      2.226      0.026       0.319       5.013\n",
       "RightAmy-RightPul    -3.9720      1.918     -2.071      0.038      -7.732      -0.212\n",
       "=====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns')\n",
    "X = data.iloc[:, 1:-4]  # just connectivities \n",
    "y = data.iloc[:, -2]   # Last column is the target variable -- mh group \n",
    "\n",
    "def logit_pvalue(model, x):\n",
    "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "    parameters:\n",
    "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "        x:     matrix on which the model was fit\n",
    "    This function uses asymtptics for maximum likelihood estimates.\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(logit_pvalue(model, X))\n",
    "\n",
    "# compare with statsmodels\n",
    "import statsmodels.api as sm\n",
    "sm_model = sm.Logit(y, sm.add_constant(X)).fit(disp=0)\n",
    "print(sm_model.pvalues)\n",
    "sm_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
