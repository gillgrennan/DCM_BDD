{
 "cells": [
  {
   "cell_type": "raw",
   "id": "895ccc8f",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "1st method: iteratively trying top features and testing model prediction accuracy \n",
    "2nd method: recursive feature selection \n",
    "3rd method: sequential feature selection (both forward / backward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97a92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4233ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy using all connectivities + MH data: 0.8571428571428571\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your data in a Pandas DataFrame called 'data'\n",
    "# where the first 4 columns are your features and the last column is the target variable.\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns_4node')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-1]  # First 4 columns are features\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the logistic regression model\n",
    "logreg = LogisticRegression(random_state = 42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg, X_train, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"Prediction Accuracy using all connectivities + MH data:\", accuracy_logreg)\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40908dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Top Features and Importance Scores:\n",
      "depression: 0.7864334575815304\n",
      "anxiety: 0.6726835747510992\n",
      "Pul-FFA: 0.6432661090019542\n",
      "Amy-FFA: 0.4192452453353923\n",
      "mOFC-Amy: 0.3554604262920525\n",
      "FFA-Pul: 0.34396275785121844\n",
      "mOFC-mOFC: 0.2498117977323696\n",
      "Amy-Pul: 0.23980172672521025\n",
      "Pul-Pul: 0.10875531445540156\n",
      "Pul-Amy: 0.10188201994018085\n"
     ]
    }
   ],
   "source": [
    "## feature selection -- reducing number of features to top importance \n",
    "\n",
    "# Step 4: Get the coefficients (feature importances) of the model\n",
    "feature_importances = np.abs(logreg.coef_[0])  # Take the absolute values to handle negative coefficients\n",
    "\n",
    "# Step 5: Select the top features with the highest importance/amount of covariance explained \n",
    "num_top_features = 10 # Change this number to select a different number of top features\n",
    "top_feature_indices = np.argsort(feature_importances)[::-1][:num_top_features]\n",
    "top_features = X.columns[top_feature_indices]\n",
    "\n",
    "# Print the selected top features and covariance explained by feature \n",
    "print(\"Selected Top Features and Importance Scores:\")\n",
    "for feature, importance in zip(top_features, feature_importances[top_feature_indices]):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d713eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.86\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Retrain the logistic regression model using only the selected top features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "logreg_top_features = LogisticRegression()\n",
    "logreg_top_features.fit(X_train_top, y_train)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_top_features, X_train_top, y_train, cv=num_folds)\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_top = logreg_top_features.predict(X_test_top)\n",
    "accuracy_logreg = accuracy_score(y_test,y_pred_top)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_top_features.score(X_test_top, y_test)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9099b0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance explained by each feature:\n",
      "depression: 0.7864334575815304\n",
      "anxiety: 0.6726835747510992\n",
      "Accuracy of logistic regression classifier on test set: 0.86\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.94\n"
     ]
    }
   ],
   "source": [
    "# testing model accuracy of just Mental Health data (anxiety / depression ratings )\n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_mhonly')\n",
    "\n",
    "X_train_mh, X_test_mh, y_train_mh, y_test_mh = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg_mh = LogisticRegression()\n",
    "logreg_mh.fit(X_train_mh, y_train_mh)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_mh, X_train_mh, y_train_mh, cv=num_folds)\n",
    "average_accuracy_mh = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_mh = logreg_mh.predict(X_test_mh)\n",
    "accuracy_logreg_mh = accuracy_score(y_test_mh,y_pred_mh)\n",
    "\n",
    "## feature selection -- reducing number of features to top importance \n",
    "\n",
    "# Step 4: Get the coefficients (feature importances) of the model\n",
    "feature_importances_mh = np.abs(logreg_mh.coef_[0])  # Take the absolute values to handle negative coefficients\n",
    "\n",
    "# Step 5: Select the top features with the highest importance/amount of covariance explained \n",
    "num_top_features = 2 # Change this number to select a different number of top features\n",
    "top_feature_indices_mh = np.argsort(feature_importances_mh)[::-1][:num_top_features]\n",
    "top_features_mh = X.columns[top_feature_indices_mh]\n",
    "\n",
    "# Print the selected top features and covariance explained by feature \n",
    "print(\"Covariance explained by each feature:\")\n",
    "for feature, importance in zip(top_features_mh, feature_importances_mh[top_feature_indices_mh]):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_mh.score(X_test_mh, y_test_mh)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy_mh:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bc2028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.86\n",
      "\n",
      "Accuracy using 10-fold Cross-Validation: 0.94\n"
     ]
    }
   ],
   "source": [
    "# testing model accuracy of mh data (anx/depression) + significantly different \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_limitedconns_4node')\n",
    "\n",
    "X_train_mhconn2, X_test_mhconn2, y_train_mhconn2, y_test_mhconn2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg_mhconn2 = LogisticRegression()\n",
    "logreg_mhconn2.fit(X_train_mhconn2, y_train_mhconn2)\n",
    "\n",
    "# Evaluate the model performance using k-fold cross-validation\n",
    "num_folds = 10  # Change this number to modify the number of folds\n",
    "cv_accuracy = cross_val_score(logreg_mhconn2, X_train_mhconn2, y_train_mhconn2, cv=num_folds)\n",
    "average_accuracy_mhconn2 = np.mean(cv_accuracy)\n",
    "\n",
    "# Evaluate prediction accuracy \n",
    "y_pred_mhconn2 = logreg_mhconn2.predict(X_test_mhconn2)\n",
    "accuracy_logreg_mhconn2 = accuracy_score(y_test_mhconn2,y_pred_mhconn2)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_mhconn2.score(X_test_mhconn2, y_test_mhconn2)))\n",
    "print(f\"\\nAccuracy using {num_folds}-fold Cross-Validation: {average_accuracy_mhconn2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73351cc7",
   "metadata": {},
   "source": [
    "# Method 2 -- recursive feature elimination w k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9beb08e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 2\n",
      "Features selected by forward sequential selection: ['Pul-FFA' 'depression']\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination with cross-validation \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns_4node')\n",
    "\n",
    "feature_names = X.columns.values\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-1]  # All but first / last column are features\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[rfecv.get_support()]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2ec696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 1\n",
      "Features selected by forward sequential selection: ['depression']\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination with cross-validation -- mh only  \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_mhonly')\n",
    "\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-1]  # All but first / last column are features\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "feature_names = X.columns.values\n",
    "\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[rfecv.get_support()]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c51fef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 1\n",
      "Features selected by forward sequential selection: ['depression']\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination with cross-validation -- mh + significantly diff connectivies   \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_limitedconns_4node')\n",
    "\n",
    "# Step 1: Import data and separate features (X) and target (y)\n",
    "X = data.iloc[:, 1:-1]  # All but first / last column are features\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "feature_names = X.columns.values\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[rfecv.get_support()]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d9760",
   "metadata": {},
   "source": [
    "# Method 3: sequential feature selection (both forward / backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1698b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by forward sequential selection: ['Amy-Amy' 'depression']\n",
      "Done in 0.443s\n",
      "Features selected by backward sequential selection: ['Pul-Pul' 'depression']\n",
      "Done in 2.449s\n"
     ]
    }
   ],
   "source": [
    "## # sequential feature selector \n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# reg_input_allconns == every single connectivity outputted by DCM (64) + anxiety / depression scores \n",
    "data = pd.read_excel('data.xlsx',sheet_name = 'reg_input_allconns_4node')\n",
    "X = data.iloc[:, 1:-1]  # 2nd - 2nd to end columns are features (ignoring subjectname + group)\n",
    "y = data.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "feature_names = X.columns.values\n",
    "\n",
    "logreg1 = LogisticRegression()\n",
    "#logreg_ridge = RidgeCV(logreg1.fit(X,y))\n",
    "# starts with no features and adds one by one \n",
    "tic_fwd = time()\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    logreg1, n_features_to_select=2, direction=\"forward\"\n",
    ").fit(X, y)\n",
    "toc_fwd = time()\n",
    "\n",
    "# backwards -- starts w all features and slowly adds each one \n",
    "tic_bwd = time()\n",
    "sfs_backward = SequentialFeatureSelector(\n",
    "    logreg1, n_features_to_select=2, direction=\"backward\"\n",
    ").fit(X, y)\n",
    "toc_bwd = time()\n",
    "\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[sfs_forward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "print(\n",
    "    \"Features selected by backward sequential selection: \"\n",
    "    f\"{feature_names[sfs_backward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
